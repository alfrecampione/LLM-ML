{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlapvalera/LLM-ML/blob/main/source/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtQB9jQj-too",
        "outputId": "87104878-7f18-49b5-8870-d67c3bd16f43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "id": "NC-nIH2A-yT9",
        "outputId": "0f727944-7531-438a-97db-ff00ebe38782"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando el dataset...\n",
            "Cargando el modelo BERT y el tokenizador...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocesando los datos...\n",
            "Preparando los dataloaders...\n",
            "Convertir los datasets a TensorDatasets\n",
            "Convertir los datasets a TensorDatasets\n",
            "Usando dispositivo: cpu\n",
            "Comenzando entrenamiento por 1 épocas...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Época 1/1\n",
            "Cargando pesos de la época anterior\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d55f61ba5e46>\u001b[0m in \u001b[0;36m<cell line: 190>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Cargando pesos de la época anterior\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1023\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m                 return _load(opened_zipfile,\n\u001b[0m\u001b[1;32m   1026\u001b[0m                              \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m                              \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1446\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1414\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1416\u001b[0;31m             \u001b[0mtyped_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m         typed_storage = torch.storage.TypedStorage(\n\u001b[0;32m-> 1390\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m             _internal=True)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    250\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertForQuestionAnswering, BertTokenizerFast, AdamW\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import TensorDataset\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Cargar el dataset SQuAD\n",
        "print(\"Cargando el dataset...\")\n",
        "dataset = load_dataset(\"squad\")\n",
        "\n",
        "# Recortar el dataset para pruebas rápidas\n",
        "#train_size = 100  # Número de ejemplos de entrenamiento\n",
        "#eval_size = 20   # Número de ejemplos de evaluación\n",
        "\n",
        "#dataset[\"train\"] = dataset[\"train\"].select(range(train_size))\n",
        "#dataset[\"validation\"] = dataset[\"validation\"].select(range(eval_size))\n",
        "\n",
        "# 2. Cargar el modelo BERT y el tokenizador\n",
        "print(\"Cargando el modelo BERT y el tokenizador...\")\n",
        "model_name = \"bert-base-uncased\"\n",
        "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "\n",
        "# 3. Preprocesar los datos\n",
        "print(\"Preprocesando los datos...\")\n",
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        stride=128,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        sample_idx = sample_map[i]\n",
        "        answer = answers[sample_idx]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label is (0, 0)\n",
        "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
        "\n",
        "# 4. Preparar los dataloaders\n",
        "print(\"Preparando los dataloaders...\")\n",
        "train_dataset = tokenized_datasets[\"train\"]\n",
        "eval_dataset = tokenized_datasets[\"validation\"]\n",
        "\n",
        "# Convertir los datasets a TensorDatasets\n",
        "def convert_to_tensordataset(dataset):\n",
        "    print(\"Convertir los datasets a TensorDatasets\")\n",
        "    return TensorDataset(\n",
        "        torch.tensor(dataset['input_ids']),\n",
        "        torch.tensor(dataset['attention_mask']),\n",
        "        torch.tensor(dataset['start_positions']),\n",
        "        torch.tensor(dataset['end_positions'])\n",
        "    )\n",
        "\n",
        "train_dataset = convert_to_tensordataset(train_dataset)\n",
        "eval_dataset = convert_to_tensordataset(eval_dataset)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=8)\n",
        "\n",
        "# 5. Configurar el entrenamiento\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# 6. Función de entrenamiento\n",
        "def process_batch(batch, device):\n",
        "    if isinstance(batch, list):\n",
        "        # Si es una lista, asumimos que los elementos están en este orden\n",
        "        input_ids, attention_mask, start_positions, end_positions = batch\n",
        "        return {\n",
        "            'input_ids': input_ids.to(device),\n",
        "            'attention_mask': attention_mask.to(device),\n",
        "            'start_positions': start_positions.to(device),\n",
        "            'end_positions': end_positions.to(device)\n",
        "        }\n",
        "    elif isinstance(batch, dict):\n",
        "        return {k: v.to(device) for k, v in batch.items()}\n",
        "    else:\n",
        "        raise ValueError(f\"Unexpected batch type: {type(batch)}\")\n",
        "\n",
        "def train(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    for batch in tqdm(dataloader, desc=\"Entrenando\"):\n",
        "        batch = process_batch(batch, device)\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "# 7. Función de evaluación\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluando\"):\n",
        "            batch = process_batch(batch, device)\n",
        "            outputs = model(**batch)\n",
        "            total_loss += outputs.loss.item()\n",
        "\n",
        "            # Obtener predicciones\n",
        "            start_logits = outputs.start_logits\n",
        "            end_logits = outputs.end_logits\n",
        "            start_pred = torch.argmax(start_logits, dim=1)\n",
        "            end_pred = torch.argmax(end_logits, dim=1)\n",
        "\n",
        "            # Aplanar las predicciones y las etiquetas\n",
        "            predictions = torch.stack((start_pred, end_pred), dim=1).view(-1).cpu().numpy()\n",
        "            labels = torch.stack((batch['start_positions'], batch['end_positions']), dim=1).view(-1).cpu().numpy()\n",
        "\n",
        "            all_predictions.extend(predictions)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "    # Calcular métricas\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
        "\n",
        "    return total_loss / len(dataloader), accuracy, f1\n",
        "\n",
        "# ... (El resto del código permanece igual hasta la sección de entrenamiento y evaluación)\n",
        "\n",
        "\n",
        "# 8. Entrenamiento y evaluación\n",
        "num_epochs = 1\n",
        "print(f\"Comenzando entrenamiento por {num_epochs} épocas...\")\n",
        "\n",
        "# Montar Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Crear un directorio en Google Drive para guardar los checkpoints\n",
        "save_dir = \"/content/drive/MyDrive/fine_tuned_bert_squad\"\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "checkpoint_path = os.path.join(save_dir, \"checkpoint.pt\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Época {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    # Cargar los pesos de la época anterior si existen\n",
        "    if epoch > -1 and os.path.exists(checkpoint_path):\n",
        "        print(f\"Cargando pesos de la época anterior\")\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    train(model, train_dataloader, optimizer, device)\n",
        "    loss, accuracy, f1 = evaluate(model, eval_dataloader, device)\n",
        "    print(f\"Pérdida de validación: {loss:.4f}\")\n",
        "    print(f\"Exactitud de validación: {accuracy:.2%}\")\n",
        "    print(f\"F1-score de validación: {f1:.4f}\")\n",
        "\n",
        "    # Guardar los pesos del modelo al final de cada época\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "        'f1': f1,\n",
        "    }, checkpoint_path)\n",
        "    print(f\"Checkpoint guardado en {checkpoint_path}\")\n",
        "\n",
        "# Guardar el modelo final en Google Drive\n",
        "print(\"Guardando el modelo final...\")\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "print(\"¡Entrenamiento completado!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzwrTn6m33BT",
        "outputId": "7df3107d-1823-4db5-94bb-cb1988f15f1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "puedes\n",
            "puedes\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import torch\n",
        "from transformers import BertForQuestionAnswering, BertTokenizerFast\n",
        "from google.colab import drive\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Montar Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Cargar el modelo y el tokenizador\n",
        "model_path = \"/content/drive/MyDrive/fine_tuned_bert_squad\"  # Ruta donde guardaste tu modelo entrenado\n",
        "model = BertForQuestionAnswering.from_pretrained(model_path)\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def answer_question(question, context):\n",
        "    # Tokenizar la pregunta y el contexto\n",
        "    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Obtener predicciones\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # Obtener las posiciones de inicio y fin de la respuesta\n",
        "    answer_start = torch.argmax(outputs.start_logits)\n",
        "    answer_end = torch.argmax(outputs.end_logits)\n",
        "\n",
        "    # Convertir las posiciones de los tokens a las posiciones de los caracteres en el contexto\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "    answer = tokenizer.convert_tokens_to_string(tokens[answer_start:answer_end+1])\n",
        "\n",
        "    # Limpiar la respuesta\n",
        "    answer = answer.replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\").strip()\n",
        "\n",
        "    return answer\n",
        "\n",
        "question = \"como se llama mi amiguito\"\n",
        "context = \"leo es una persomna,Este ejemplo muestra cómo crear una API simple usando Flask para utilizar tu modelo BERT entrenado para responder preguntas. La API acepta solicitudes POST con una pregunta y un contexto, y devuelve la respuesta generada por el modelo. Puedes extender esta API para incluir más funcionalidades según tus necesidades,leo es mi amigo\"\n",
        "answer =answer_question(question, context)\n",
        "print(answer)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    question = \"como se llama mi amiguito\"\n",
        "    context = \"leo es una persomna,Este ejemplo muestra cómo crear una API simple usando Flask para utilizar tu modelo BERT entrenado para responder preguntas. La API acepta solicitudes POST con una pregunta y un contexto, y devuelve la respuesta generada por el modelo. Puedes extender esta API para incluir más funcionalidades según tus necesidades,leo es mi amigo\"\n",
        "    print(answer_question(question, context))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt5Sb4VjGrpU"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqJIYULvbZRq",
        "outputId": "c2bcae65-0953-4de3-f1c3-7edcc5fb747f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: neo4j in /usr/local/lib/python3.10/dist-packages (5.22.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j) (2023.4)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.35.12)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download es_core_news_sm\n",
        "!pip install neo4j\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DERZwBKiBAir",
        "outputId": "0c488b4e-efe3-4b32-af4e-a585e50efea2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original:\n",
            "Este es un texto de ejemplo que se utilizará para generar un resumen usando el modelo T5. El resumen debe capturar las ideas principales del texto de manera concisa y clara.\n",
            "\n",
            "Resumen:\n",
            "es un texto de ejemplo que se utilizará para generar un resumen usando el modelo T5.\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Cargar el modelo y el tokenizador\n",
        "model_name = \"t5-small\"  # Puedes usar \"t5-base\" o \"t5-large\" para mejores resultados, pero requieren más recursos\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "def summarize_text(text, max_length=100):\n",
        "    # Preparar el input\n",
        "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "    # Generar el resumen\n",
        "    summary_ids = model.generate(inputs, max_length=max_length, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Decodificar y retornar el resumen\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Ejemplo de uso\n",
        "text = \"Este es un texto de ejemplo que se utilizará para generar un resumen usando el modelo T5. El resumen debe capturar las ideas principales del texto de manera concisa y clara.\"\n",
        "summary = summarize_text(text)\n",
        "print(\"Texto original:\")\n",
        "print(text)\n",
        "print(\"\\nResumen:\")\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhsIqfZZLwyk"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "RBB_t-CmY6sO"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import threading\n",
        "\n",
        "def get_similarity(query:str,vector_space:list, k:int) -> list[tuple[str,float]]: #TODO\n",
        "    \"\"\"\n",
        "    Finds the k most relevant texts in a set of texts given a query text.\n",
        "\n",
        "    Parameters:\n",
        "        - query (str): The query text.\n",
        "        - vector_space (list): A list of texts to search.\n",
        "        - k (int): The number of most relevant texts to return.\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples (text, score) where text is one of the k most\n",
        "        relevant texts and score is its degree of similarity to the query text.\n",
        "    \"\"\"\n",
        "    if vector_space==[]:\n",
        "        return []\n",
        "    # Create TF-IDF vectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "    # Ajustar y transformar el texto de consulta y el conjunto de textos\n",
        "    X = vectorizer.fit_transform([query] + vector_space)\n",
        "\n",
        "    # Calcular la similitud de coseno entre el texto de consulta y cada texto en el conjunto\n",
        "    similarities = cosine_similarity(X[0], X[1:])\n",
        "\n",
        "    # Ordenar los textos por similitud de coseno en orden descendente\n",
        "    sorted_indices = similarities.argsort()[0][::-1]\n",
        "\n",
        "    # Devolver los k textos más relevantes\n",
        "    return [(vector_space[i], similarities[0][i]) for i in sorted_indices[:k]]\n",
        "\n",
        "\n",
        "class LTM_Node:\n",
        "    '''\n",
        "        A node in the LTM network.\n",
        "        There is two kind of nodes:\n",
        "            -The actual past conversations:\n",
        "                These kind of nodes cannot have any children\n",
        "            -The summary nodes:\n",
        "                This kind has any number of children in a dictionary (vector->node)\n",
        "                His vector is just a summary of his children vectors\n",
        "    '''\n",
        "\n",
        "    def __init__(self, arg:str|list['LTM_Node'],is_leaf=True):\n",
        "\n",
        "        if is_leaf:\n",
        "            '''\n",
        "                This represent a new memory in LTM\n",
        "                conversation: the conversation that this node represents\n",
        "                parents: a list of the nodes that are parents of this node\n",
        "            '''\n",
        "            self.vector:str = arg\n",
        "            self.children: dict[str,'LTM_Node']|None = None\n",
        "            self.is_leaf:bool = True\n",
        "            self.parents:list['LTM_Node'] = []\n",
        "            return\n",
        "\n",
        "        '''\n",
        "            This represent a summary of memories\n",
        "            MemoryList: a list of the memories that this node summarizes (children nodes)\n",
        "            parents: a list of the nodes that are parents of this node\n",
        "        '''\n",
        "        self.children: dict[str,'LTM_Node']|None = {x.vector:x for x in arg}\n",
        "        self.vector:str = self.calculate_vector()\n",
        "        self.is_leaf:bool=False\n",
        "        self.parents=[]\n",
        "\n",
        "    def insert(self,node:'LTM_Node'):\n",
        "        '''\n",
        "            Adds a new children. This node(self) cannot be a memory node\n",
        "        '''\n",
        "        if self.children==None:\n",
        "            raise Exception(\"LTM Insertion Error\")\n",
        "        else:\n",
        "            self.children[node.vector]=node\n",
        "            node.parents.append(self)\n",
        "        self.update()\n",
        "\n",
        "    def mix_memories(self, vector:str, new_memory_node:'LTM_Node'):\n",
        "        '''\n",
        "            Mix a new memory with a child in a new summary node,\n",
        "            The new summary will be son of this node instead of the mixed child\n",
        "            - vector: the vector of the child that will be mixed\n",
        "            - new_memory_node: the new memory node\n",
        "        '''\n",
        "        old_child = self.children[vector]\n",
        "        new_summary=LTM_Node([old_child,new_memory_node], False)\n",
        "        new_memory_node.parents.append(new_summary)\n",
        "        old_child.parents.remove(self)\n",
        "        old_child.parents.append(new_summary)\n",
        "        self.children.pop(vector)\n",
        "        self.insert(new_summary)\n",
        "\n",
        "    def update(self):\n",
        "        '''\n",
        "            Update the vector of this node and the parents of this node\n",
        "        '''\n",
        "        if self.children==None:\n",
        "            return\n",
        "        self.children={x.vector:x for _, x in self.children.items()}\n",
        "        self.vector=self.calculate_vector()\n",
        "        for p in self.parents:\n",
        "            p.update()\n",
        "\n",
        "    def get_space(self) -> list[str]|None:\n",
        "        '''\n",
        "            Return all the children vectors\n",
        "        '''\n",
        "        if self.is_leaf:\n",
        "            return None\n",
        "        return list(self.children.keys())\n",
        "\n",
        "    def calculate_vector(self) -> str:\n",
        "        \"\"\"\n",
        "        Calculate a summary of this node's children using the summarize_text method\n",
        "        \"\"\"\n",
        "        if not self.children:\n",
        "            return \"No hay información para resumir.\"\n",
        "\n",
        "        # Combine all texts from children\n",
        "        combined_text = \" \".join([node.vector for node in self.children.values()])\n",
        "        # Use the summarize_text method to generate a summary of the combined texts\n",
        "        summary = summarize_text(combined_text, max_length=150)\n",
        "\n",
        "        return summary\n",
        "\n",
        "class LTM:\n",
        "    '''\n",
        "        This class represents the LTM of the model.\n",
        "        Contains a LTM_Node graph starting with the root node.\n",
        "        This structure has 2 hyper-parameters:\n",
        "            - _lambda: the minimum level of relevance that a vector must have for a query\n",
        "            - k_child: the number of vectors retrieved to a query\n",
        "        The hyper parameter 'k_child' will also be relevant for the graph construction:\n",
        "            - For k=1: The graph will be a tree\n",
        "            - For k>1: The graph will be a DAG\n",
        "    '''\n",
        "\n",
        "    def __init__(self, _lambda:float=0.5, k_child:int=1):\n",
        "        self.root=LTM_Node([], False)\n",
        "        self._lambda=_lambda\n",
        "        self.k_child=k_child\n",
        "        self.where_to_insert=[]\n",
        "        #NOTE: THE VARIABLE 'where_to_insert' IS USED TO STORAGE A NEW MEMORY,\n",
        "        # BASED ON THE SEARCH RESULTS HAS THE STRUCTURE [(node,vector)], WHERE:\n",
        "        #       - node will be the father of the new memory\n",
        "        #       - vector will be the most relevant vector\n",
        "\n",
        "    def insert(self,new_memory:str):\n",
        "        '''\n",
        "            Insert a new memory in the LTM.\n",
        "\n",
        "            It will be inserted on the nodes that were relevant for the prompt\n",
        "            stored in 'where_to_insert' variable updated in 'get_vector' method.\n",
        "        '''\n",
        "        new_node = LTM_Node(new_memory)\n",
        "\n",
        "        for node, vector in self.where_to_insert:\n",
        "            if node.vector==vector and not vector in node.children.keys():\n",
        "                #NOTE: Situation 1: The most relevant node was a summary.\n",
        "                node.insert(new_node)\n",
        "            else:\n",
        "                #NOTE: Situation 2: The most relevant node was another memory.\n",
        "                node.mix_memories(vector,new_node)\n",
        "\n",
        "        self.where_to_insert=[]\n",
        "\n",
        "    def get_vector(self, prompt:str) -> list[str]:\n",
        "        '''\n",
        "            Return the first k (from 'k_child' variable) most relevant vectors for the prompt\n",
        "\n",
        "            Also, save the nodes that will be the parents of the new memory and the relevant vectors\n",
        "            for new memory insertion.\n",
        "        '''\n",
        "\n",
        "        self.where_to_insert = [\n",
        "            (node, vector) for node, vector, _ in\n",
        "            self.relevant_nodes(self.root, prompt,self._lambda)\n",
        "        ]\n",
        "\n",
        "        if len(self.where_to_insert)==0:\n",
        "            #NOTE: CASE OF NO RELEVANT MEMORIES\n",
        "            self.where_to_insert=[(self.root,self.root.vector)]\n",
        "            return ['']\n",
        "\n",
        "        return [vector for _,vector in self.where_to_insert]\n",
        "\n",
        "    def relevant_nodes(self, node:LTM_Node, prompt:str,\n",
        "        _lambda:float, solution=None) -> list[tuple[LTM_Node,str,float]]:\n",
        "        '''\n",
        "            Receives a LTM, a prompt and a lambda value\n",
        "            Return a list of tuple (node, vector, similarity) where:\n",
        "            - node is the node that gives the future parent vector of the memory\n",
        "            - vector is the vector that was relevant for the prompt (greater than lambda)\n",
        "            - similarity is the similarity between the vector and the prompt\n",
        "            The tuple is also stored in solution\n",
        "        '''\n",
        "        if solution==None:\n",
        "            solution=[]\n",
        "\n",
        "        best_results = get_similarity(prompt, node.get_space(), self.k_child)\n",
        "\n",
        "        #The following 3 list are made for a summary node. The i-est element represents each child\n",
        "        child_node=[]\n",
        "        child_threads=[]\n",
        "        child_relevant_nodes=[]\n",
        "\n",
        "        for vector, similarity in best_results:\n",
        "\n",
        "            if node.children[vector].is_leaf:\n",
        "                # On this case the future parent of the memory cannot be the vector itself.\n",
        "                # So, we will return the parent of the vector and the memories will\n",
        "                # be mixed into a new summary that will be storage in the parent.\n",
        "                if similarity>_lambda:\n",
        "                    solution.append((node,vector, similarity))\n",
        "                continue\n",
        "\n",
        "            # If the relevant vector is a summary, we return the best result between this vector and\n",
        "            # the recursive call on this vector. This process will be running on a thread\n",
        "            child_node.append((node.children[vector],vector,similarity))\n",
        "            child_relevant_nodes.append([])\n",
        "            child_threads.append(threading.Thread(target=self.relevant_nodes,\n",
        "                args=(node.children[vector], prompt,max(_lambda,similarity), child_relevant_nodes[-1])))\n",
        "            child_threads[-1].start()\n",
        "\n",
        "        #Joining the threads\n",
        "        for i in range(len(child_threads)):\n",
        "            child_threads[i].join()\n",
        "            _,_,similarity = child_node[i]\n",
        "            if len(child_relevant_nodes[i])!=0:\n",
        "                solution.append(child_relevant_nodes[i][0])\n",
        "            elif similarity > _lambda:\n",
        "                solution.append(child_node[i])\n",
        "\n",
        "        solution.sort(key = lambda x:x[2], reverse=True)\n",
        "        return solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_4kIXUTYeZR",
        "outputId": "bd45e5e8-7808-460f-b2c2-dc6bf520576a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<function ShortTermMemory.__init__.<locals>.<lambda> at 0x7bd620a58940>, {'apple': [2, 1720595954.5432353], 'banana': [2, 1720595954.5432634], 'cherry': [1, 1720595954.5432873]})\n",
            "defaultdict(<function ShortTermMemory.__init__.<locals>.<lambda> at 0x7bd620a58940>, {'apple': [3, 1720595954.5432353], 'banana': [3, 1720595954.5432634], 'cherry': [2, 1720595954.5432873]})\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import time\n",
        "\n",
        "class ShortTermMemory:\n",
        "    def __init__(self, max_size=100, update_frequency=10, auto_delete_frequency=20):\n",
        "        self.max_size = max_size\n",
        "        self.update_frequency = update_frequency\n",
        "        self.auto_delete_frequency = auto_delete_frequency\n",
        "        self.memory = defaultdict(lambda: [0, time.time()])\n",
        "\n",
        "    def clear(self):\n",
        "        self.memory=defaultdict(lambda: [0, time.time()])\n",
        "\n",
        "    def add(self, item):\n",
        "        if item in self.memory:\n",
        "            self.memory[item][0] += 1\n",
        "            if self.memory[item][0] % self.auto_delete_frequency == 0:\n",
        "                del self.memory[item]\n",
        "        else:\n",
        "            self.memory[item] = [1, time.time()]\n",
        "            self._prune_memory()\n",
        "\n",
        "    def _prune_memory(self):\n",
        "        if len(self.memory) > self.max_size:\n",
        "            oldest_items = sorted(self.memory.items(), key=lambda x: x[1][1])\n",
        "            for item, (count, timestamp) in oldest_items[:-self.max_size]:\n",
        "                del self.memory[item]\n",
        "\n",
        "    def update(self):\n",
        "        current_time = time.time()\n",
        "        for item, (count, timestamp) in list(self.memory.items()):\n",
        "            if count % self.update_frequency == 0:\n",
        "                self.memory[item][1] = current_time\n",
        "            self.memory[item][0] += 1\n",
        "            if self.memory[item][0] % self.auto_delete_frequency == 0:\n",
        "                del self.memory[item]\n",
        "        self._prune_memory()\n",
        "\n",
        "# Ejemplo de uso\n",
        "short_term_memory = ShortTermMemory(max_size=50, update_frequency=5, auto_delete_frequency=10)\n",
        "\n",
        "short_term_memory.add(\"apple\")\n",
        "short_term_memory.add(\"banana\")\n",
        "short_term_memory.add(\"cherry\")\n",
        "short_term_memory.add(\"apple\")\n",
        "short_term_memory.add(\"banana\")\n",
        "\n",
        "print(short_term_memory.memory)\n",
        "# Output: defaultdict(<function ShortTermMemory.<lambda>.<locals>.<lambda> at 0x7f6a8c0c8d60>, {'apple': [2, 1619540400.0], 'banana': [2, 1619540400.0], 'cherry': [1, 1619540400.0]})\n",
        "\n",
        "short_term_memory.update()\n",
        "print(short_term_memory.memory)\n",
        "# Output: defaultdict(<function ShortTermMemory.<lambda>.<locals>.<lambda> at 0x7f6a8c0c8d60>, {'apple': [3, 1619540405.0], 'banana': [3, 1619540405.0]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "EevIfA85Ztpq"
      },
      "outputs": [],
      "source": [
        "# Creación de una instancia de LTM\n",
        "ltm = LTM(_lambda=0.01, k_child=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "iHM7qZQVuwUR"
      },
      "outputs": [],
      "source": [
        "# Función para obtener memoria relevante\n",
        "def get_relevant_short_term_memory(prompt: str):\n",
        "    answer = []\n",
        "    for k, v in short_term_memory.memory:\n",
        "        answer.append(answer_question(prompt, k))\n",
        "    return answer\n",
        "\n",
        "def concatenate_strings(string_list, user_input):\n",
        "    \"\"\"\n",
        "    Concatena una lista de strings.\n",
        "\n",
        "    Parámetros:\n",
        "    - string_list (list): La lista de strings a concatenar.\n",
        "    - user_input (str): La entrada del usuario a añadir al final.\n",
        "\n",
        "    Devuelve:\n",
        "    - str: El resultado de concatenar todos los strings en la lista y la entrada del usuario.\n",
        "    \"\"\"\n",
        "    result = \"\"\n",
        "    for string in string_list:\n",
        "        result += string\n",
        "    result += user_input\n",
        "    return result\n",
        "def update_short_term_memory(user_input, response, short_term_memory):\n",
        "    # Añadir la entrada del usuario a la memoria a corto plazo\n",
        "    short_term_memory.add(user_input)\n",
        "\n",
        "    # Añadir la respuesta del LLM a la memoria a corto plazo\n",
        "    short_term_memory.add(response)\n",
        "\n",
        "    # Actualizar la memoria a corto plazo\n",
        "    short_term_memory.update()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN816y013xpz",
        "outputId": "f170b635-fc2e-469e-b20f-4608a4b00407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "xbOyJtEV3-Y5",
        "outputId": "ee964837-11af-45e4-d85f-062eb2275bd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ingresa tu pregunta: asd\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "get_relevant_short_term_memory() missing 1 required positional argument: 'short_term_memory'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-df87ac7c6974>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Obtener las memorias relevantes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mrelevant_short_term_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_relevant_short_term_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0mrelevant_long_term_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_relevant_long_term_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: get_relevant_short_term_memory() missing 1 required positional argument: 'short_term_memory'"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "class LLM:\n",
        "    def __init__(self):\n",
        "        self.model_name = \"gpt2\"\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(self.model_name)\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(self.model_name)\n",
        "\n",
        "    def generate_response(self, prompt, max_length=500):\n",
        "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "        attention_mask = torch.ones(inputs.shape, dtype=torch.long)\n",
        "        outputs = self.model.generate(inputs,\n",
        "                                      max_length=max_length,\n",
        "                                      num_return_sequences=1,\n",
        "                                      attention_mask=attention_mask,\n",
        "                                      no_repeat_ngram_size=2,\n",
        "                                      temperature=0.7)\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return response\n",
        "\n",
        "def create_prompt(user_input, relevant_short_term_memory, relevant_long_term_memory):\n",
        "    K_shot='''\n",
        "    Contexto: En Francia, sus ciudades son Lyon y su capital Paris\\n\n",
        "    Pregunta: Cual es la capital de Francia\\n\n",
        "    Respuesta: Paris\\n\\n\n",
        "\n",
        "    Contexto: Los mejores pintores del renacimiento fueron muchos\\n\n",
        "    Pregunta: Quien pinto la mona lisa\\n\n",
        "    Respuesta: Leonardo Da Vinci\\n\\n\n",
        "\n",
        "    '''\n",
        "\n",
        "    context = f\"Contexto: {relevant_short_term_memory}\\n\"\n",
        "    context += f\"{relevant_long_term_memory}\\n\"\n",
        "    prompt = f\"{context}\\nPregunta: {user_input}\\nRespuesta: \"\n",
        "    return prompt\n",
        "\n",
        "def get_relevant_short_term_memory(user_input, short_term_memory):\n",
        "    # Aquí iría tu lógica para obtener la memoria a corto plazo relevante\n",
        "    answer = []\n",
        "    for k, v in short_term_memory.memory:\n",
        "        answer.append(k)\n",
        "    return '. '.join(answer)\n",
        "\n",
        "\n",
        "# Inicializar el modelo\n",
        "llm = LLM()\n",
        "\n",
        "# Uso\n",
        "user_input = input(\"Ingresa tu pregunta: \")\n",
        "\n",
        "# Obtener las memorias relevantes\n",
        "relevant_short_term_memory = get_relevant_short_term_memory(user_input)\n",
        "relevant_long_term_memory = get_relevant_long_term_memory(user_input)\n",
        "\n",
        "# Crear el prompt y generar la respuesta\n",
        "prompt = create_prompt(user_input, relevant_short_term_memory, relevant_long_term_memory)\n",
        "response = llm.generate_response(prompt)\n",
        "\n",
        "print(\"Respuesta generada:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HnbdY1rZVRev",
        "outputId": "78cef111-38d1-401f-9f7e-8a41544dd87b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "defaultdict(<function ShortTermMemory.clear.<locals>.<lambda> at 0x7bd623ae2a70>, {})\n",
            "Ingresa tu pregunta: Eres mi entrenador físico. Quiero tener una rutina de gimnasio. Que necesitas saber?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contexto: \n",
            "\n",
            "\n",
            "Pregunta: Eres mi entrenador físico. Quiero tener una rutina de gimnasio. Que necesitas saber?\n",
            "Respuesta: \n",
            "\n",
            "Quieros a la vida de la fiesta.\n",
            "\n",
            "\n",
            " (P)\n",
            "The following is a translation of the following text from the Spanish translation by the author:\n",
            "\"The Spanish language is the language of all the peoples of this world. It is not the only language, but it is also the most important. The Spanish people are the people of every country, and they are not only the best of mankind, they also are also of great importance to the whole world.\"\n",
            "(Pablo de las Casas, \"The Language of Man\")\n",
            "I am not sure if this is correct, or if it was meant to be a statement of fact. I am sure that it's not. But I think it would be interesting to know what the meaning of \"the language\" is.\n",
            "\n",
            "...\n",
            "....\n",
            "\n",
            "\n",
            "\n",
            "The language that is spoken in the world is that of man. And the man who speaks it, is called man, because he is man in his own right. He is his master, his servant, the master of his people. In the same way, he who is in charge of a people is responsible for the actions of that people, for their actions, their deeds, as well as for all their lives. This is what is meant by \"man\" in this context. Man is an individual, a person, who has a mind, an understanding, which is to say, of an idea, that which he has. That is, man is able to understand the idea of what he wants to do, to think about what his life is like, what it means to him, how he feels about it. So, in order to make a man a master in a society, you have to have a sense of how you want to live. You have the sense that you are a human being, not a machine. If you don't have that sense, then you will not be able, if you do not have it in you, your life will be miserable. Therefore, it will take you a long time to get to that point. There is no way to escape that. When you get there, there is nothing you can do. Your life depends on it.\"\n",
            "\n",
            "I think that this translation is very accurate. However, I would like to ask you to read the translation from Spanish. What is it about the words that make you think of them?\n",
            "\n",
            "A. They are words of meaning. A word is something that has meaning, meaning that can be expressed in words. For example, when you say \"I want a car\", you mean \"to drive a vehicle\". You can express that meaning in any way you like. As a matter of course, this means that the car is going to go to a certain place, so you must express it as a meaning to drive. Also, we can say that a word can have meaning if we express its meaning as an expression of something. We can also say it can mean anything. Thus, words can come from any place. (p. 5)\n",
            "\n",
            "In the case of words, these are things that are expressed by them. These are called words because they express the meanings of things. Words are expressions of ideas. Ideas are ideas that have meanings. To express a concept, one has to express something, such as \"this is my house\", \"that is how I like it\", etc. All of these expressions are meant for expressing the ideas of people in general. One can use these words to describe a situation, say something about a place or a thing, etc., and then express them in such a way that they can become a part of one's life. Here is another example. \"This is where I live\". \"It is here that I work\". These expressions can all be used to convey a feeling of being in control of your own life, something you cannot express in other words or in another way. Now, let's say you live in an apartment. How do you express this feeling in your apartment? You say: \"Here is this place\". This means: this apartment is your home. Then, after you leave, go back to your house. Do you feel that way? Do not feel like you belong there. Instead, do what you need to. Say: I want this car. Why do I need this? Because I have this idea. Because it has this meaning and because I feel it to me. Let's take a look at the English translation. English is used in many different ways. Some people use it for a specific purpose, like a job, some for an education, others for entertainment. Others use the word \"expression\" to\n",
            "Ingresa tu pregunta: Soy un hombre de 22 años, mido 1.78 metros, de complexion delgada. No padezco de problemas severos de salud que me impidan hacer ejercicio. Llevo cerca de 8 meses iendo al gimnasio. Dispongo de 5 días a la semana para entrenar y mi objetivo es obtener masa muscular.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1113 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error al obtener el contexto: too many values to unpack (expected 2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input length of input_ids is 1113, but `max_length` is set to 1000. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-03c659623911>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_short_term_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_long_term_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-df87ac7c6974>\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(self, prompt, max_length)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         outputs = self.model.generate(inputs,\n\u001b[0m\u001b[1;32m     14\u001b[0m                                       \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                       \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1646\u001b[0m                 \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"past_key_values\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_static_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_generated_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_default_max_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m         \u001b[0;31m# 7. determine generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_validate_generated_length\u001b[0;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_ids_length\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0minput_ids_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"decoder_input_ids\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1177\u001b[0m                 \u001b[0;34mf\"Input length of {input_ids_string} is {input_ids_length}, but `max_length` is set to\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m                 \u001b[0;34mf\" {generation_config.max_length}. This can lead to unexpected behavior. You should consider\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input length of input_ids is 1113, but `max_length` is set to 1000. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."
          ]
        }
      ],
      "source": [
        "# 1. Inicializar el modelo\n",
        "llm = LLM()\n",
        "short_term_memory = ShortTermMemory(max_size=3, update_frequency=3, auto_delete_frequency=3)\n",
        "short_term_memory.clear()\n",
        "long_term_memory = LTM(_lambda=0.01, k_child=1)\n",
        "\n",
        "print(short_term_memory.memory)\n",
        "a=100\n",
        "\n",
        "while True:\n",
        "    # 2. Obtener la entrada del usuario\n",
        "    user_input = input(\"Ingresa tu pregunta: \")\n",
        "\n",
        "    if user_input=='~~~':\n",
        "        break\n",
        "\n",
        "    # 3. Obtener el contexto relevante\n",
        "    try:\n",
        "        relevant_short_term_memory = get_relevant_short_term_memory(user_input,short_term_memory)\n",
        "    except Exception as e:\n",
        "        print(\"Error al obtener el contexto:\", str(e))\n",
        "    # 3. Extraer lo más relevante de la memoria a largo plazo\n",
        "\n",
        "    relevant_long_term_memory='. '.join(long_term_memory.get_vector(context+' '+user_input))\n",
        "\n",
        "    # 4. Usar todo lo extraído como contexto para generar una respuesta con el LLM\n",
        "\n",
        "    prompt = create_prompt(user_input, relevant_short_term_memory, relevant_long_term_memory)\n",
        "\n",
        "    response = llm.generate_response(prompt,1000)\n",
        "    a+=150\n",
        "\n",
        "    # 5. Actualizar la memoria a corto plazo con la respuesta del LLM\n",
        "    update_short_term_memory(user_input, response, short_term_memory)\n",
        "    # 6. Insertar la respuesta en la memoria a largo plazo\n",
        "    long_term_memory.insert(response)\n",
        "\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ltm=LTM(0.01,1)\n",
        "while True:\n",
        "    user_input = input(\"Ingresa tu pregunta: \")\n",
        "    if user_input=='~~~':\n",
        "        break\n",
        "    ltm.get_vector(user_input)\n",
        "    ltm.insert(user_input)\n",
        "\n",
        "queque=[ltm.root]\n",
        "while queque!=[]:\n",
        "  a=queque[0]\n",
        "  queque.remove(a)\n",
        "  print(a.vector)\n",
        "  print(a.get_space())\n",
        "  print(a.parents.vector)\n",
        "  if a.is_leaf:\n",
        "    continue\n",
        "  for x in a.children:\n",
        "    queque.append(a.children[x])"
      ],
      "metadata": {
        "id": "tcg_piCPlvvu",
        "outputId": "fdb7ce6a-14ca-4e93-f921-43f780abf854",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ingresa tu pregunta: Eres mi entrenador físico. Quiero tener una rutina de gimnasio. Que necesitas saber?\n",
            "Ingresa tu pregunta: Soy un hombre de 22 años, mido 1.78 metros, de complexion delgada. No padezco de problemas severos de salud que me impidan hacer ejercicio. Llevo cerca de 8 meses iendo al gimnasio. Dispongo de 5 días a la semana para entrenar y mi objetivo es obtener masa muscular.\n",
            "Ingresa tu pregunta: Por favor, mi gimnasio no tiene muchos equipos, es bastante modesto, trata de evitar lo mas posible las maquinas especializadas en mi entrenamiento.\n",
            "Ingresa tu pregunta: Dame una rutina donde el lunes sea pecho y triceps, el martes sea biceps y abdomen, miércoles piernas, jueves hombro y viernes espalda.\n",
            "Ingresa tu pregunta: Para cada dia dame al menos 5 ejercicios diferentes.\n",
            "Ingresa tu pregunta: Mis sesiones de entrenamiento son por la mañana, que me recomiendas desayunar?\n",
            "Ingresa tu pregunta: No tengo dinero para comprar suplementos como proteínas y cosas asi, que alimentos me sugieres para intercambiarlos.\n",
            "Ingresa tu pregunta: Adoro la carne, pero no me gustan los vegetales, hay alguna opción saludable por la cual intercambiar los vegetales?\n",
            "Ingresa tu pregunta: Mi mejor amigo Ian Carlos me sugiere llevar jugo de naranja en vez de agua al gimnasio. Crees que sea buena idea tomar jugo entre sesiones?\n",
            "Ingresa tu pregunta: Si bien los domingos por la mañana voy a la iglesia con mis padres, que actividades me recomiendas practicar por la tarde del domingo con mis amigos que me aporten a tener mejor cuerpo\n",
            "Ingresa tu pregunta: Últimamente tengo un leve dolor en la muñeca, edita mi plan de entrenamiento para evitar ejercicios que requieran mucho esfuerzo en esta.\n",
            "Ingresa tu pregunta: Los días de hombros me gustaría combinarlos con algún deporte, me gustan los deportes en equipos como el baloncesto o de pelea como el boxeo, cual me recomiendas?\n",
            "Ingresa tu pregunta: En mi casa tengo cosas como faja o guantes. Que cosas de ese tipo consideras que deba llevar al gimnasio?\n",
            "Ingresa tu pregunta: Ana Karla, la hermana de Ian Carlos, se nos unirá un dia que otro, que dia de la semana seria mas efectivo que se nos uniera?\n",
            "Ingresa tu pregunta: Este viernes voy a una fiesta con Carla para festejar que aprobamos machine learning, y eso hace preguntarme, que puedo hacer para mover los ejercicios de ese dia o compensar que no los hare de cierta manera?\n",
            "Ingresa tu pregunta: ~~~\n",
            "me gustan los deportes en equipos como el baloncesto? Dame una rutina donde el lunes sea pecho y triceps, miércoles piernas, jueves hombro y viernes espalda?\n",
            "['me gustan los deportes en equipos como el baloncesto? Dame una rutina donde el lunes sea pecho y triceps, el martes sea biceps y abdomen, miércoles piernas, jueves hombro y viernes espalda?']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'vector'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-5c1a72cc6a38>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'vector'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "queque=[ltm.root]\n",
        "dic=dict()\n",
        "c=0\n",
        "while queque!=[]:\n",
        "  a=queque[0]\n",
        "  dic[a.vector]=c\n",
        "  c+=1\n",
        "  queque.remove(a)\n",
        "  print(f\"node: {dic[a.vector]}, parents:{[dic(x.vector) for x in a.parents]}\")\n",
        "  if a.is_leaf:\n",
        "    continue\n",
        "  for x in a.children:\n",
        "    queque.append(a.children[x])"
      ],
      "metadata": {
        "id": "HpEUshuGrgjB",
        "outputId": "ab3ac618-b076-49ba-d059-72606377ed2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "me gustan los deportes en equipos como el baloncesto? Dame una rutina donde el lunes sea pecho y triceps, miércoles piernas, jueves hombro y viernes espalda?\n",
            "['me gustan los deportes en equipos como el baloncesto? Dame una rutina donde el lunes sea pecho y triceps, el martes sea biceps y abdomen, miércoles piernas, jueves hombro y viernes espalda?']\n",
            "[]\n",
            "me gustan los deportes en equipos como el baloncesto? Dame una rutina donde el lunes sea pecho y triceps, el martes sea biceps y abdomen, miércoles piernas, jueves hombro y viernes espalda?\n",
            "['me gustan los deportes en equipos como el baloncesto o de pelea como el boxeo?', 'Dame una rutina donde el lunes sea pecho y triceps, el martes sea biceps y abdomen, miércoles piernas, jueves hombro y viernes espalda.']\n",
            "['me gustan los deportes en equipos como el baloncesto? Dame una rutina donde el lunes sea pecho y triceps, miércoles piernas, jueves hombro y viernes espalda?']\n",
            "me gustan los deportes en equipos como el baloncesto o de pelea como el boxeo?\n",
            "['mi gimnasio no tiene muchos equipos, es bastante modesto, trata de evitar lo mas posible las maquinas especializadas en mi entrenamiento.', 'me gustan los deportes en equipos como el baloncesto o de pelea como el boxeo? ese tipo consideras que deba llevar al gimnasio?']\n",
            "['me gustan los deportes en equipos como el baloncesto? Dame una rutina donde el lunes sea pecho y triceps, el martes sea biceps y abdomen, miércoles piernas, jueves hombro y viernes espalda?']\n",
            "Dame una rutina donde el lunes sea pecho y triceps, el martes sea biceps y abdomen, miércoles piernas, jueves hombro y viernes espalda.\n",
            "['Eres mi entrenador físico. Quiero tener una rutina de gimnasio. Que necesitas saber?', 'Dame una rutina donde el lunes sea pecho y triceps, el martes sea biceps y abdomen, miércoles piernas, jueves hombro y viernes espalda.']\n",
            "['me gustan los deportes en equipos como el baloncesto? Dame una rutina donde el lunes sea pecho y triceps, el martes sea biceps y abdomen, miércoles piernas, jueves hombro y viernes espalda?']\n",
            "mi gimnasio no tiene muchos equipos, es bastante modesto, trata de evitar lo mas posible las maquinas especializadas en mi entrenamiento.\n",
            "['Por favor, mi gimnasio no tiene muchos equipos, es bastante modesto, trata de evitar lo mas posible las maquinas especializadas en mi entrenamiento.', 'Mi mejor amigo Ian Carlos me sugiere llevar jugo de naranja en vez de agua al gimnasio. Crees que sea buena idea tomar jugo entre sesiones?', 'Ana Karla, la hermana de Ian Carlos, se nos unirá un dia que otro, que dia de la semana seria mas efectivo que se nos uniera?']\n",
            "['me gustan los deportes en equipos como el baloncesto o de pelea como el boxeo?']\n",
            "me gustan los deportes en equipos como el baloncesto o de pelea como el boxeo? ese tipo consideras que deba llevar al gimnasio?\n",
            "['no tengo dinero para comprar suplementos como protenas y cosas asi. me gustan los deportes en equipos como el baloncesto o de pelea como el boxeo?', 'ese tipo consideras que deba llevar al gimnasio? Este viernes voya una fiesta con Carla para festejar que aprobamos machine learning, y eso hacer para mover los ejercicios de ese dia o compensar que no los hare de cierta manera?']\n",
            "['me gustan los deportes en equipos como el baloncesto o de pelea como el boxeo?']\n",
            "Eres mi entrenador físico. Quiero tener una rutina de gimnasio. Que necesitas saber?\n",
            "None\n",
            "['Dame una rutina donde el lunes sea pecho y triceps, el martes sea biceps y abdomen, miércoles piernas, jueves hombro y viernes espalda.']\n",
            "Dame una rutina donde el lunes sea pecho y triceps, el martes sea biceps y abdomen, miércoles piernas, jueves hombro y viernes espalda.\n",
            "['Dame una rutina donde el lunes sea pecho y triceps, el martes sea biceps y abdomen, miércoles piernas, jueves hombro y viernes espalda.', 'Para cada dia dame al menos 5 ejercicios diferentes.']\n",
            "['Dame una rutina donde el lunes sea pecho y triceps, el martes sea biceps y abdomen, miércoles piernas, jueves hombro y viernes espalda.']\n",
            "Por favor, mi gimnasio no tiene muchos equipos, es bastante modesto, trata de evitar lo mas posible las maquinas especializadas en mi entrenamiento.\n",
            "None\n",
            "['mi gimnasio no tiene muchos equipos, es bastante modesto, trata de evitar lo mas posible las maquinas especializadas en mi entrenamiento.']\n",
            "Mi mejor amigo Ian Carlos me sugiere llevar jugo de naranja en vez de agua al gimnasio. Crees que sea buena idea tomar jugo entre sesiones?\n",
            "None\n",
            "['mi gimnasio no tiene muchos equipos, es bastante modesto, trata de evitar lo mas posible las maquinas especializadas en mi entrenamiento.']\n",
            "Ana Karla, la hermana de Ian Carlos, se nos unirá un dia que otro, que dia de la semana seria mas efectivo que se nos uniera?\n",
            "['Últimamente tengo un leve dolor en la muñeca, edita mi plan de entrenamiento para evitar ejercicios que requieran mucho esfuerzo en esta.', 'Ana Karla, la hermana de Ian Carlos, se nos unirá un dia que otro, que dia de la semana seria mas efectivo que se nos uniera?']\n",
            "['mi gimnasio no tiene muchos equipos, es bastante modesto, trata de evitar lo mas posible las maquinas especializadas en mi entrenamiento.']\n",
            "no tengo dinero para comprar suplementos como protenas y cosas asi. me gustan los deportes en equipos como el baloncesto o de pelea como el boxeo?\n",
            "['no tengo dinero para comprar suplementos como protenas y cosas asi, que alimentos me sugieres para intercambiarlos.', 'me gustan los deportes en equipos como el baloncesto o de pelea como el boxeo?']\n",
            "['me gustan los deportes en equipos como el baloncesto o de pelea como el boxeo? ese tipo consideras que deba llevar al gimnasio?']\n",
            "ese tipo consideras que deba llevar al gimnasio? Este viernes voya una fiesta con Carla para festejar que aprobamos machine learning, y eso hacer para mover los ejercicios de ese dia o compensar que no los hare de cierta manera?\n",
            "['En mi casa tengo cosas como faja o guantes. Que cosas de ese tipo consideras que deba llevar al gimnasio?', 'Este viernes voy a una fiesta con Carla para festejar que aprobamos machine learning, y eso hace preguntarme, que puedo hacer para mover los ejercicios de ese dia o compensar que no los hare de cierta manera?']\n",
            "['me gustan los deportes en equipos como el baloncesto o de pelea como el boxeo? ese tipo consideras que deba llevar al gimnasio?']\n",
            "Dame una rutina donde el lunes sea pecho y triceps, el martes sea biceps y abdomen, miércoles piernas, jueves hombro y viernes espalda.\n",
            "None\n",
            "['Dame una rutina donde el lunes sea pecho y triceps, el martes sea biceps y abdomen, miércoles piernas, jueves hombro y viernes espalda.']\n",
            "Para cada dia dame al menos 5 ejercicios diferentes.\n",
            "None\n",
            "['Dame una rutina donde el lunes sea pecho y triceps, el martes sea biceps y abdomen, miércoles piernas, jueves hombro y viernes espalda.']\n",
            "Últimamente tengo un leve dolor en la muñeca, edita mi plan de entrenamiento para evitar ejercicios que requieran mucho esfuerzo en esta.\n",
            "None\n",
            "['Ana Karla, la hermana de Ian Carlos, se nos unirá un dia que otro, que dia de la semana seria mas efectivo que se nos uniera?']\n",
            "Ana Karla, la hermana de Ian Carlos, se nos unirá un dia que otro, que dia de la semana seria mas efectivo que se nos uniera?\n",
            "None\n",
            "['Ana Karla, la hermana de Ian Carlos, se nos unirá un dia que otro, que dia de la semana seria mas efectivo que se nos uniera?']\n",
            "no tengo dinero para comprar suplementos como protenas y cosas asi, que alimentos me sugieres para intercambiarlos.\n",
            "['no padezco de problemas severos de salud que me impidan hacer ejercicio. meses iendo al gimnasio.', 'No tengo dinero para comprar suplementos como proteínas y cosas asi, que alimentos me sugieres para intercambiarlos.']\n",
            "['no tengo dinero para comprar suplementos como protenas y cosas asi. me gustan los deportes en equipos como el baloncesto o de pelea como el boxeo?']\n",
            "me gustan los deportes en equipos como el baloncesto o de pelea como el boxeo?\n",
            "['Adoro la carne, pero no me gustan los vegetales, hay alguna opción saludable por la cual intercambiar los vegetales?', 'Si bien los domingos por la mañana voy a la iglesia con mis padres, que actividades me recomiendas practicar por la tarde del domingo con mis amigos que me aporten a tener mejor cuerpo', 'Los días de hombros me gustaría combinarlos con algún deporte, me gustan los deportes en equipos como el baloncesto o de pelea como el boxeo, cual me recomiendas?']\n",
            "['no tengo dinero para comprar suplementos como protenas y cosas asi. me gustan los deportes en equipos como el baloncesto o de pelea como el boxeo?']\n",
            "En mi casa tengo cosas como faja o guantes. Que cosas de ese tipo consideras que deba llevar al gimnasio?\n",
            "None\n",
            "['ese tipo consideras que deba llevar al gimnasio? Este viernes voya una fiesta con Carla para festejar que aprobamos machine learning, y eso hacer para mover los ejercicios de ese dia o compensar que no los hare de cierta manera?']\n",
            "Este viernes voy a una fiesta con Carla para festejar que aprobamos machine learning, y eso hace preguntarme, que puedo hacer para mover los ejercicios de ese dia o compensar que no los hare de cierta manera?\n",
            "None\n",
            "['ese tipo consideras que deba llevar al gimnasio? Este viernes voya una fiesta con Carla para festejar que aprobamos machine learning, y eso hacer para mover los ejercicios de ese dia o compensar que no los hare de cierta manera?']\n",
            "no padezco de problemas severos de salud que me impidan hacer ejercicio. meses iendo al gimnasio.\n",
            "['Soy un hombre de 22 años, mido 1.78 metros, de complexion delgada. No padezco de problemas severos de salud que me impidan hacer ejercicio. Llevo cerca de 8 meses iendo al gimnasio. Dispongo de 5 días a la semana para entrenar y mi objetivo es obtener masa muscular.', 'Mis sesiones de entrenamiento son por la mañana, que me recomiendas desayunar?']\n",
            "['no tengo dinero para comprar suplementos como protenas y cosas asi, que alimentos me sugieres para intercambiarlos.']\n",
            "No tengo dinero para comprar suplementos como proteínas y cosas asi, que alimentos me sugieres para intercambiarlos.\n",
            "None\n",
            "['no tengo dinero para comprar suplementos como protenas y cosas asi, que alimentos me sugieres para intercambiarlos.']\n",
            "Adoro la carne, pero no me gustan los vegetales, hay alguna opción saludable por la cual intercambiar los vegetales?\n",
            "None\n",
            "['me gustan los deportes en equipos como el baloncesto o de pelea como el boxeo?']\n",
            "Si bien los domingos por la mañana voy a la iglesia con mis padres, que actividades me recomiendas practicar por la tarde del domingo con mis amigos que me aporten a tener mejor cuerpo\n",
            "None\n",
            "['me gustan los deportes en equipos como el baloncesto o de pelea como el boxeo?']\n",
            "Los días de hombros me gustaría combinarlos con algún deporte, me gustan los deportes en equipos como el baloncesto o de pelea como el boxeo, cual me recomiendas?\n",
            "None\n",
            "['me gustan los deportes en equipos como el baloncesto o de pelea como el boxeo?']\n",
            "Soy un hombre de 22 años, mido 1.78 metros, de complexion delgada. No padezco de problemas severos de salud que me impidan hacer ejercicio. Llevo cerca de 8 meses iendo al gimnasio. Dispongo de 5 días a la semana para entrenar y mi objetivo es obtener masa muscular.\n",
            "None\n",
            "['no padezco de problemas severos de salud que me impidan hacer ejercicio. meses iendo al gimnasio.']\n",
            "Mis sesiones de entrenamiento son por la mañana, que me recomiendas desayunar?\n",
            "None\n",
            "['no padezco de problemas severos de salud que me impidan hacer ejercicio. meses iendo al gimnasio.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z86FUxDwcHr",
        "outputId": "c798c6a9-14d0-4042-bf78-e5fb643a64e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE-1: 1.0\n",
            "BLEU-1: 1.0\n",
            "METEOR: 0.5\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import threading\n",
        "\n",
        "def get_similarity(query: str, vector_space: list, k: int) -> list[tuple[str, float]]:\n",
        "    \"\"\"\n",
        "    Finds the k most relevant texts in a set of texts given a query text.\n",
        "\n",
        "    Parameters:\n",
        "    - query (str): The query text.\n",
        "    - vector_space (list): A list of texts to search.\n",
        "    - k (int): The number of most relevant texts to return.\n",
        "\n",
        "    Returns:\n",
        "    A list of tuples (text, score) where text is one of the k most\n",
        "    relevant texts and score is its degree of similarity to the query text.\n",
        "    \"\"\"\n",
        "    if vector_space == []:\n",
        "        return []\n",
        "\n",
        "    # Create TF-IDF vectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "    # Adjust and transform the query text and the set of texts\n",
        "    X = vectorizer.fit_transform([query] + vector_space)\n",
        "\n",
        "    # Calculate the cosine similarity between the query text and each text in the set\n",
        "    similarities = cosine_similarity(X[0], X[1:])\n",
        "\n",
        "    # Sort the texts by cosine similarity in descending order\n",
        "    sorted_indices = similarities.argsort()[0][::-1]\n",
        "\n",
        "    # Return the k most relevant texts\n",
        "    return [(vector_space[i], similarities[0][i]) for i in sorted_indices[:k]]\n",
        "\n",
        "def ROUGE(query: str, reference: str, n: int = 1) -> float:\n",
        "    \"\"\"\n",
        "    Calculates the ROUGE score for a query and a reference text.\n",
        "\n",
        "    Parameters:\n",
        "    - query (str): The query text.\n",
        "    - reference (str): The reference text.\n",
        "    - n (int): The number of n-grams to consider (default is 1).\n",
        "\n",
        "    Returns:\n",
        "    The ROUGE score.\n",
        "    \"\"\"\n",
        "    # Tokenize the query and reference texts\n",
        "    query_tokens = query.split()\n",
        "    reference_tokens = reference.split()\n",
        "\n",
        "    # Calculate the number of common n-grams\n",
        "    common_ngrams = 0\n",
        "    for i in range(len(query_tokens) - n + 1):\n",
        "        query_ngram = ' '.join(query_tokens[i:i + n])\n",
        "        for j in range(len(reference_tokens) - n + 1):\n",
        "            reference_ngram = ' '.join(reference_tokens[j:j + n])\n",
        "            if query_ngram == reference_ngram:\n",
        "                common_ngrams += 1\n",
        "                break\n",
        "\n",
        "    # Calculate the ROUGE score\n",
        "    rouge_score = common_ngrams / (len(query_tokens) - n + 1)\n",
        "\n",
        "    return rouge_score\n",
        "\n",
        "def BLEU(query: str, reference: str, n: int = 1) -> float:\n",
        "    \"\"\"\n",
        "    Calculates the BLEU score for a query and a reference text.\n",
        "\n",
        "    Parameters:\n",
        "    - query (str): The query text.\n",
        "    - reference (str): The reference text.\n",
        "    - n (int): The number of n-grams to consider (default is 1).\n",
        "\n",
        "    Returns:\n",
        "    The BLEU score.\n",
        "    \"\"\"\n",
        "    # Tokenize the query and reference texts\n",
        "    query_tokens = query.split()\n",
        "    reference_tokens = reference.split()\n",
        "\n",
        "    # Calculate the number of common n-grams\n",
        "    common_ngrams = 0\n",
        "    for i in range(len(query_tokens) - n + 1):\n",
        "        query_ngram = ' '.join(query_tokens[i:i + n])\n",
        "        for j in range(len(reference_tokens) - n + 1):\n",
        "            reference_ngram = ' '.join(reference_tokens[j:j + n])\n",
        "            if query_ngram == reference_ngram:\n",
        "                common_ngrams += 1\n",
        "                break\n",
        "\n",
        "    # Calculate the BLEU score\n",
        "    bleu_score = common_ngrams / (len(query_tokens) - n + 1)\n",
        "\n",
        "    return bleu_score\n",
        "\n",
        "def METEOR(query: str, reference: str) -> float:\n",
        "    \"\"\"\n",
        "    Calculates the METEOR score for a query and a reference text.\n",
        "\n",
        "    Parameters:\n",
        "    - query (str): The query text.\n",
        "    - reference (str): The reference text.\n",
        "\n",
        "    Returns:\n",
        "    The METEOR score.\n",
        "    \"\"\"\n",
        "    # Tokenize the query and reference texts\n",
        "    query_tokens = query.split()\n",
        "    reference_tokens = reference.split()\n",
        "\n",
        "    # Calculate the number of common n-grams\n",
        "    common_ngrams = 0\n",
        "    for i in range(len(query_tokens)):\n",
        "        query_word = query_tokens[i]\n",
        "        for j in range(len(reference_tokens)):\n",
        "            reference_word = reference_tokens[j]\n",
        "            if query_word == reference_word:\n",
        "                common_ngrams += 1\n",
        "                break\n",
        "\n",
        "    # Calculate the METEOR score\n",
        "    meteor_score = common_ngrams / (len(query_tokens) + len(reference_tokens))\n",
        "\n",
        "    return meteor_score\n",
        "\n",
        "# Ejemplo de uso\n",
        "query = \"This is an example sentence.\"\n",
        "reference = \"This is an example sentence.\"\n",
        "n = 1\n",
        "\n",
        "print(\"ROUGE-1:\", ROUGE(query, reference, n))\n",
        "print(\"BLEU-1:\", BLEU(query, reference, n))\n",
        "print(\"METEOR:\", METEOR(query, reference))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK4O3JzhKxOG",
        "outputId": "f993c549-f755-4d67-c93c-2991bd63f5fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 1.68\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertForMaskedLM, BertTokenizer\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load the model and tokenizer\n",
        "#model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Prepare the input text\n",
        "input_text = \"This is an example sentence.\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "# Calculate the perplexity\n",
        "output = model(input_ids, labels=input_ids)\n",
        "loss = output.loss\n",
        "perplexity = torch.exp(loss)\n",
        "\n",
        "print(f\"Perplexity: {perplexity.item():.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "cntR8ugxMWdm",
        "outputId": "b3b94704-70e5-4ded-a6dc-6b76f72942f6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected input batch_size (7) to match target batch_size (8).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-c755b55dd1c6>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m  \u001b[0;31m# Distribución de palabras predicha por el modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mcross_entropy_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Cross-Entropy: {cross_entropy_value:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-c755b55dd1c6>\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(p, q)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mfloat\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mcross\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mentropy\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \"\"\"\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mcross_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3086\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (7) to match target batch_size (8)."
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def cross_entropy(p, q):\n",
        "    \"\"\"\n",
        "    Calculates the cross-entropy between two probability distributions.\n",
        "\n",
        "    Parameters:\n",
        "    p (torch.Tensor): The predicted probability distribution.\n",
        "    q (torch.Tensor): The reference probability distribution.\n",
        "\n",
        "    Returns:\n",
        "    float: The cross-entropy value.\n",
        "    \"\"\"\n",
        "    loss = F.cross_entropy(p.view(-1, p.size(-1)), q.view(-1))\n",
        "    cross_entropy = loss.item()\n",
        "    return cross_entropy\n",
        "\n",
        "input_text = \"This is an example sentence.\"\n",
        "reference_text = \"This is a reference sentence.\"\n",
        "\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "reference_ids = tokenizer.encode(reference_text, return_tensors='pt')\n",
        "output = model(input_ids, labels=input_ids)\n",
        "p = output.logits  # Distribución de palabras predicha por el modelo\n",
        "cross_entropy_value = cross_entropy(p, reference_ids)\n",
        "print(f\"Cross-Entropy: {cross_entropy_value:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fOB4flANiMa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}