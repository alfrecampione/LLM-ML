<table>
    <thead>
        <tr>
            <th>Nombre</th>
            <th>Autores</th>
            <th>Año</th>
            <th>Objetivo/Propósito</th>
            <th>Metodología</th>
            <th>Resultados</th>
            <th>Conclusiones</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>MEMORY NETWORKS</td>
            <td>Jason Weston, Sumit Chopra & Antoine Bordes</td>
            <td>29-11-2015</td>
            <td>
                Introducir una clase de modelos poderosos llamados "memory networks" y mostrar una instancia de estos para la tarea de respuesta a preguntas.
            </td>
            <td>
                Presentan la arquitectura general de los "memory networks" y sus componentes clave (I, G, O, R). Exploran variantes como el uso de hashing de memoria para mejorar la eficiencia. Realizan experimentos en tareas de preguntas y respuestas a gran escala y en un mundo simulado.
            </td>
            <td>
                Los "memory networks" muestran un buen desempeño en las tareas de preguntas y respuestas evaluadas. El uso de hashing de memoria permite obtener grandes mejoras en eficiencia sin perder mucho en rendimiento.
            </td>
            <td>
                Los autores concluyen que los "memory networks" son una clase de modelos poderosos que deben explorarse más a fondo en tareas de comprensión de texto y otras áreas. Identifican varias direcciones futuras, como probar en tareas más complejas, explorar configuraciones débilmente supervisadas y desarrollar arquitecturas más sofisticadas.
            </td>
        </tr>
        <tr>
            <td>MemoryBank: Enhancing Large Language Models with Long-Term Memory</td>
            <td>Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang</td>
            <td>21-5-2023</td>

            <td>
                Abordar la limitación de memoria a corto plazo de los Modelos de Lenguaje Grande (LLM) mediante la introducción de MemoryBank, una arquitectura que dota a los LLM de memoria a largo plazo.
                Evaluar la eficacia de MemoryBank en un escenario de compañero de IA a largo plazo llamado SiliconFriend.
                Demostrar las capacidades de MemoryBank para mejorar la empatía, el discernimiento y la comprensión de la personalidad del usuario en las interacciones de SiliconFriend.
            </td>
            <td>
                Se diseña e implementa MemoryBank, una arquitectura que incorpora un almacén de memoria externa a los LLM.
                Se desarrolla SiliconFriend, un chatbot basado en LLM potenciado por MemoryBank.
                Se recopilan diálogos reales con usuarios para evaluar el rendimiento cualitativo de SiliconFriend.
                Se generan diálogos simulados para evaluar el rendimiento cuantitativo de SiliconFriend.
                Se utilizan métricas como la coherencia conversacional, la relevancia del tema y la comprensión de la personalidad del usuario para evaluar el rendimiento de SiliconFriend.
            </td>
            <td>
                Se propone MemoryBank, una arquitectura que integra un almacén de memoria externa en los LLM para facilitar el almacenamiento y la recuperación de información a largo plazo.
                Se implementa SiliconFriend, un chatbot basado en LLM potenciado por MemoryBank, diseñado para funcionar como compañero de IA a largo plazo.
                Se evalúa el rendimiento de SiliconFriend en dos tipos de análisis:

                Análisis cualitativo: Se examinan diálogos reales con usuarios para evaluar la empatía, el discernimiento y la comprensión de la personalidad del usuario por parte de SiliconFriend.
                Análisis cuantitativo: Se utilizan diálogos simulados para evaluar la capacidad de SiliconFriend para mantener conversaciones coherentes y relevantes a lo largo del tiempo.
            </td>
            <td>
                MemoryBank demuestra ser una arquitectura eficaz para dotar a los LLM de memoria a largo plazo, mejorando su capacidad para mantener conversaciones coherentes y relevantes a lo largo del tiempo.
                SiliconFriend, potenciado por MemoryBank, exhibe una empatía, un discernimiento y una comprensión de la personalidad del usuario mejoradas en comparación con los LLM tradicionales.
                Los resultados sugieren que la integración de memoria a largo plazo en los LLM tiene el potencial de mejorar significativamente sus capacidades para interacciones a largo plazo, como el compañerismo de IA, la consejería psicológica y las tareas de secretaría.
            </td>
        </tr>
        

        <tr>
            <td>RecallM: An Adaptable Memory Mechanism with Temporal Understanding for Large Language Models</td>
            <td>Brandon Kynoch, Hugo Latapie, Dwane van der Sluis</td>
            <td>3-10-2023</td>

            <td>
                Introducir RecallM, un mecanismo de memoria adaptable con comprensión temporal para modelos de lenguaje grande (LLM).
                Mejorar la capacidad de los LLM para recordar y utilizar información relevante en contextos temporales extensos.
                Evaluar el rendimiento de RecallM en una variedad de tareas de procesamiento del lenguaje natural (PLN) que requieren comprensión temporal, como responder preguntas, generación de texto y resumen.
            </td>
            <td>
                Se propone RecallM, una arquitectura de memoria que combina un mecanismo de memoria externa con un módulo de atención temporal.
                RecallM se implementa en el marco de trabajo Transformer y se integra en un LLM de última generación.
                Se evalúa el rendimiento de RecallM en seis tareas de PLN: responder preguntas abiertas, responder preguntas de respuesta única, resumen, traducción automática, generación de texto y comprensión de lectura.
                Se comparan los resultados de RecallM con los de modelos de línea base que no utilizan mecanismos de memoria.
            </td>
            <td>
                Se demuestra que RecallM mejora significativamente el rendimiento de los LLM en las seis tareas de PLN evaluadas.
                RecallM logra una mayor precisión en responder preguntas, generación de texto y resumen.
                RecallM mejora la fluidez y la coherencia en la traducción automática y la generación de texto.
                RecallM demuestra una mejor comprensión de la información temporal en la comprensión de lectura.
            </td>
            <td>
                RecallM es un mecanismo de memoria eficaz que mejora la capacidad de los LLM para recordar y utilizar información relevante en contextos temporales extensos.
                RecallM tiene el potencial de ser una herramienta valiosa para el desarrollo de sistemas de PLN de última generación.
                Los resultados sugieren que la integración de mecanismos de memoria con comprensión temporal puede mejorar significativamente el rendimiento de los LLM en una variedad de tareas de PLN.
            </td>
        </tr>
        
        <tr>
            <td>"My agent understands me better": Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents</td>
            <td>Yuki Hou, Haruki Tamoto, Homei Miyashita</td>
            <td>31-3-2024</td>

            <td>
                Mejorar la memoria de los agentes de diálogo basados en LLM: Los autores buscan superar las limitaciones de los LLM en cuanto a la memoria y la capacidad para mantener conversaciones coherentes.
                Implementar una arquitectura de memoria inspirada en la memoria humana: La propuesta se basa en la recuperación de información relevante mediante señales de memoria basadas en la relevancia y el tiempo transcurrido.
                Permitir a los agentes recordar información de forma autónoma: El objetivo es que los agentes puedan acceder y utilizar información de interacciones pasadas para generar respuestas más precisas y eficientes.
            </td>
            <td>
                Arquitectura de memoria con recuperación basada en señales: Se describe una arquitectura que incluye un mecanismo de recuperación de memoria activado por señales que indican la relevancia de la información y el tiempo transcurrido desde su almacenamiento.
                Algoritmo de consolidación de memoria: Se implementa un algoritmo para consolidar la información en la memoria a largo plazo, mejorando su retención y accesibilidad.
                Evaluación experimental
            </td>
            <td>
                Mejora en la precisión de la recuperación de memoria: Los resultados preliminades muestran que la arquitectura propuesta mejora la precisión de la recuperación de memoria en comparación con métodos tradicionales.
                Reducción en el tiempo de respuesta: La recuperación de información relevante mediante señales de memoria permite a los agentes generar respuestas de manera más eficiente.
                Conversaciones más coherentes: La capacidad de recordar información de interacciones pasadas conduce a diálogos más fluidos y naturales.
            </td>
            <td>
                La arquitectura de memoria propuesta ofrece un enfoque prometedor para mejorar la memoria de los agentes LLM.
                La recuperación de memoria basada en señales y la consolidación de memoria a largo plazo son clave para lograr una interacción humano-computadora más natural y efectiva.
                Se requieren investigaciones adicionales para evaluar la escalabilidad y la generalización de la arquitectura a diferentes dominios y tareas.
            </td>
        </tr>
    </tbody>
</table>

